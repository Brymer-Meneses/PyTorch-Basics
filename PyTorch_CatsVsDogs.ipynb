{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch - CatsVsDogs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMYSDkDVoCt9Ex7+zDxUFpy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brymer-Meneses/PyTorch-Basics/blob/master/PyTorch_CatsVsDogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzFImKcavmE"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p40PJHgxaDVU"
      },
      "source": [
        "!wget https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWB4zzT2afgi"
      },
      "source": [
        "from zipfile import ZipFile"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr9aqUSYaeVb"
      },
      "source": [
        "zipPath = '/content/kagglecatsanddogs_3367a.zip'\n",
        "zip = ZipFile(zipPath, 'r')\n",
        "zip.extractall()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxuMviqdbcGy"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7lyxrv9bYUo",
        "outputId": "4f8eb964-5ca8-4de6-8760-7961b454dbbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "\n",
        "num_skipped = 0\n",
        "for folder_name in (\"Cat\", \"Dog\"):\n",
        "    folder_path = os.path.join(\"PetImages\", folder_name)\n",
        "    for fname in os.listdir(folder_path):\n",
        "        fpath = os.path.join(folder_path, fname)\n",
        "        try:\n",
        "            fobj = open(fpath, \"rb\")\n",
        "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
        "        finally:\n",
        "            fobj.close()\n",
        "\n",
        "        if not is_jfif:\n",
        "            num_skipped += 1\n",
        "            # Delete corrupted image\n",
        "            os.remove(fpath)\n",
        "\n",
        "print(\"Deleted %d images\" % num_skipped)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleted 1590 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL0NHNRwa4wy"
      },
      "source": [
        "# Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVf4hE8Oap3T"
      },
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clVTZthRb1f_"
      },
      "source": [
        "path = '/content/PetImages'\n",
        "dataset = ImageFolder(path)\n",
        "\n",
        "trainData, testData, trainLabel, testLabel = train_test_split(dataset.imgs, dataset.targets, test_size =0.2, random_state = 0)\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize((200,200)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM2-aXLUb9qK"
      },
      "source": [
        "class ImageLoader(Dataset):\n",
        "  def __init__(self, dataset, transform=None):\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    image = Image.open(self.dataset[item][0])\n",
        "    if transform is not None:\n",
        "      return self.transform(image), self.dataset[item][1]\n",
        "    return image, self.dataset[item][1]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "  \n",
        "  def checkChannel(self, dataset):\n",
        "    datasetRGB = []\n",
        "    for index in range(len(dataset)):\n",
        "      if Image.open(dataset[index][0]).getbands() == ('R', 'G', 'B'):\n",
        "        datasetRGB.append(dataset[index])\n",
        "    return datasetRGB\n",
        "  \n",
        "  def getResizedImage(self, item):\n",
        "    image = Image.open(self.dataset[item][0])\n",
        "    _, _, width, height = image.getbbox()\n",
        "\n",
        "    factor = (0,0, width, width) if width > height else (0,0,height, height)\n",
        "\n",
        "    return image.crop(factor)\n",
        "\n",
        "\n",
        "imageLoader = ImageLoader(trainData, transform)\n",
        "\n",
        "dataLoader = DataLoader(imageLoader, batch_size = 10, shuffle = True)\n",
        "data= iter(dataLoader)\n",
        "d = next(data)\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EicaR5HXtayQ"
      },
      "source": [
        "from torch.nn import Module, Conv2d, Linear, Flatten, MaxPool2d\n",
        "from torch.nn.functional import relu"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcZQCb0bsvuP"
      },
      "source": [
        "class Network(Module):\n",
        "  def __init__(self):\n",
        "    super(Network, self).__init__()\n",
        "    self.conv_1 = Conv2d(in_channels = 3, out_channels = 64, kernel_size = 5)\n",
        "    self.conv_2 = Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)\n",
        "    self.conv_3 = Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3)\n",
        "\n",
        "    self.maxPooling = MaxPool2d(kernel_size =4)\n",
        "\n",
        "    self.fc1 = Linear(in_features = 256, out_features = 128)\n",
        "    self.fc2 = Linear(in_features = 128, out_features =64)\n",
        "    self.out = Linear(in_features = 64, out_features = 2)\n",
        "  \n",
        "  def forward(self):\n",
        "    x = self.conv_1(x)\n",
        "    x = self.maxPooling(x)\n",
        "    x = relu(x)\n",
        "\n",
        "    x = self.conv_2(x)\n",
        "    x = self.maxPooling(x)\n",
        "    x = relu(x)\n",
        "\n",
        "    x = self.conv_3(x)\n",
        "    x = self.maxPooling(x)\n",
        "    x = relu(x)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = relu(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "    x = relu(x)\n",
        "\n",
        "    return self.out(x)"
      ],
      "execution_count": 53,
      "outputs": []
    }
  ]
}